{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced CNN Stock Market Prediction\n",
    "\n",
    "Improvements over the original model:\n",
    "1. Additional technical indicators\n",
    "2. Enhanced CNN architecture with batch normalization\n",
    "3. Better training process with validation\n",
    "4. Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(data):\n",
    "    # Calculate moving averages\n",
    "    data['MA5'] = data['Close'].rolling(window=5).mean()\n",
    "    data['MA20'] = data['Close'].rolling(window=20).mean()\n",
    "    \n",
    "    # Calculate RSI\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Calculate MACD\n",
    "    exp1 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = exp1 - exp2\n",
    "    \n",
    "    # Calculate Bollinger Bands\n",
    "    data['BB_middle'] = data['Close'].rolling(window=20).mean()\n",
    "    data['BB_upper'] = data['BB_middle'] + 2 * data['Close'].rolling(window=20).std()\n",
    "    data['BB_lower'] = data['BB_middle'] - 2 * data['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # Forward fill NaN values\n",
    "    return data.fillna(method='ffill')\n",
    "\n",
    "# Fetch data with longer period for better indicator calculation\n",
    "stock_symbol = \"RELIANCE.NS\"\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=100)  # 100 days for better indicator calculation\n",
    "\n",
    "data = yf.download(stock_symbol, start=start_date.strftime('%Y-%m-%d'), \n",
    "                  end=end_date.strftime('%Y-%m-%d'), interval=\"30m\")\n",
    "\n",
    "# Add technical indicators\n",
    "data = add_technical_indicators(data)\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"\\nFeatures:\", list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=15):\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Select features\n",
    "        self.feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                               'MA5', 'MA20', 'RSI', 'MACD',\n",
    "                               'BB_middle', 'BB_upper', 'BB_lower']\n",
    "        \n",
    "        self.data = data[self.feature_columns].values\n",
    "        \n",
    "        # Scale the data\n",
    "        self.scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        self.scaled_data = self.scaler.fit_transform(self.data)\n",
    "        \n",
    "        # Create sequences\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for i in range(len(self.data) - sequence_length):\n",
    "            sequence = self.scaled_data[i:(i + sequence_length)]\n",
    "            target = self.scaled_data[i + sequence_length, 3]  # Close price\n",
    "            self.sequences.append(sequence)\n",
    "            self.targets.append(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.sequences[idx]), \n",
    "                torch.FloatTensor([self.targets[idx]]))\n",
    "\n",
    "# Create dataset\n",
    "dataset = EnhancedDataset(data)\n",
    "\n",
    "# Split into train and validation sets (80-20)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, input_channels=12, hidden_size1=128, hidden_size2=64, dropout_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=hidden_size1, kernel_size=(3,3), \n",
    "                     stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=1),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            \n",
    "            nn.Conv2d(in_channels=hidden_size1, out_channels=hidden_size2, \n",
    "                     kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=1),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            \n",
    "            nn.Conv2d(in_channels=hidden_size2, out_channels=32, \n",
    "                     kernel_size=(2,2), stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and training components\n",
    "model = EnhancedCNN(input_channels=len(dataset.feature_columns))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                       factor=0.5, patience=5, \n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sequences, targets in train_loader:\n",
    "        sequences = sequences.unsqueeze(1)  # Add channel dimension\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences = sequences.unsqueeze(1)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'enhanced_stock_prediction_model.pth')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.6f}')\n",
    "    print(f'Val Loss: {val_loss:.6f}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler for future predictions\n",
    "import joblib\n",
    "joblib.dump(dataset.scaler, 'enhanced_stock_scaler.pkl')\n",
    "\n",
    "# Make a prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get the last sequence from our data\n",
    "    last_sequence = dataset.scaled_data[-15:]\n",
    "    sequence_tensor = torch.FloatTensor(last_sequence).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model(sequence_tensor)\n",
    "    \n",
    "    # Convert prediction back to original scale\n",
    "    dummy_array = np.zeros((1, len(dataset.feature_columns)))\n",
    "    dummy_array[0, 3] = prediction.item()  # Close price index\n",
    "    predicted_price = dataset.scaler.inverse_transform(dummy_array)[0, 3]\n",
    "    \n",
    "    # Get actual price\n",
    "    actual_price = data['Close'].iloc[-1]\n",
    "    \n",
    "    print(f'Predicted Price: ₹{predicted_price:.2f}')\n",
    "    print(f'Actual Price: ₹{actual_price:.2f}')\n",
    "    print(f'Prediction Error: ₹{abs(actual_price - predicted_price):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
